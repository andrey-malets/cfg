class { 'apt':
    always_apt_update    => false,
    disable_keys         => undef,
    purge_sources_list   => false,
    purge_sources_list_d => false,
    purge_preferences_d  => false,
    update_timeout       => undef
}

class slurm {
    package { 'munge':      ensure => installed }
    package { 'slurm-llnl': ensure => installed }

    file { '/etc/munge/munge.key':
        require => Package['munge'],
        notify => Service['munge'],
        mode => 400,
        owner => munge,
        group => munge,
        source => "puppet:///files/munge.key",
    }

    service { 'munge':
        ensure => 'running',
        enable => 'true',
        require => File['/etc/munge/munge.key'],
    }

    file { '/etc/slurm-llnl/slurm.conf':
        require => Package['slurm-llnl'],
        notify => Service['slurm-llnl'],
        mode => 644,
        owner => root,
        group => root,
        source => "puppet:///files/slurm.conf",
    }

    service { 'slurm-llnl':
        require => Service['munge'],
        ensure => 'running',
        enable => 'true',
    }
}

class ssh {
    package { 'openssh-server': ensure => installed }
    package { 'openssh-client': }

    service { 'ssh':
        ensure  => 'running',
        enable  => 'true',
    }

    augeas { 'ssh_config':
        require => Package['openssh-client'],
        context => '/files/etc/ssh/ssh_config',
        changes => ['set Host/ForwardAgent yes'],
    }

    augeas { 'sshd_config':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        context => '/files/etc/ssh/sshd_config',
        changes => ['set UseDNS no',
                    'set AllowAgentForwarding yes'],
    }

    if $userqwer {
        augeas { 'sshd_config_userqwer':
            require => Package['openssh-server'],
            notify => Service['ssh'],
            context => '/files/etc/ssh/sshd_config',
            changes => 'set DenyUsers/1 user',
        }
    }
}

class managed-ssh inherits ssh {
    file { '/etc/ssh/ssh_host_dsa_key':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        mode => 600,
        owner => root,
        group => root,
        source => "puppet:///$hostname/ssh_host_dsa_key",
    }

    file { '/etc/ssh/ssh_host_dsa_key.pub':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        mode => 644,
        owner => root,
        group => root,
        source => "puppet:///$hostname/ssh_host_dsa_key.pub",
    }

    file { '/etc/ssh/ssh_host_rsa_key':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        mode => 600,
        owner => root,
        group => root,
        source => "puppet:///$hostname/ssh_host_rsa_key",
    }

    file { '/etc/ssh/ssh_host_rsa_key.pub':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        mode => 644,
        owner => root,
        group => root,
        source => "puppet:///$hostname/ssh_host_rsa_key.pub",
    }

    augeas { 'managed_sshd_config':
        require => Package['openssh-server'],
        notify => Service['ssh'],
        context => '/files/etc/ssh/sshd_config',
        changes => ['set AuthorizedKeysFile "/etc/ssh/keys/%u /root/.ssh/authorized_keys"'],
    }

    file { '/etc/ssh/keys.tgz':
        require => Package['openssh-server'],
        mode => 600,
        owner => root,
        group => root,
        source => 'puppet:///files/keys.tgz',
    }

    file { '/usr/local/bin/ssh.sh':
        mode => 700,
        owner => root,
        group => root,
        content => "#!/bin/bash
            KEYDIR=/etc/ssh/keys
            mkdir -p \$KEYDIR && cd \$KEYDIR && rm -rf *
            chown root.root . && chmod 755 .
            tar xf /etc/ssh/keys.tgz
            chmod 600 *
            ",
    }

    exec { '/usr/local/bin/ssh.sh':
        subscribe => [File['/etc/ssh/keys.tgz'],
                      File['/usr/local/bin/ssh.sh']],
        refreshonly => true,
    }
}

class managed-update {
    file { '/root/empty.sh':
        mode => 700,
        owner => root,
        group => root,
        content => "#!/bin/bash
            [[ $(w -h | grep -v '^user$' | wc -l) -eq 0 ]] && \
            [[ $(pgrep tmux | wc -l) -eq 0 ]] && \
            [[ $(find /var/run/screen -type p | wc -l) -eq 0 ]]
        ",
    }

    cron { 'managed-update':
        command => '/root/empty.sh && /sbin/init 6',
        user => root,
        hour => 7,
        minute => 10,
        require => File['/root/empty.sh'],
    }
}

class managed-apt {
    package { 'sudo':
        ensure => installed,
    }

    file { '/etc/sudoers.d/managed-apt':
        require => Package['sudo'],
        ensure => present,
        mode => '440',
        owner => root,
        group => root,
        content => 'ALL ALL=(ALL:ALL) /usr/bin/apt-get update
                    ALL ALL=(ALL:ALL) /usr/bin/apt-get install -- *
                   ',
    }
}

class runc_ca {
    file { 'dijkstra.urgu.org.crt':
        path => '/usr/local/share/ca-certificates/dijkstra.urgu.org.crt',
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/ca.crt',
        require => Package['ca-certificates'],
    }

    exec { '/usr/sbin/update-ca-certificates -f':
        subscribe => File['dijkstra.urgu.org.crt'],
        refreshonly => true,
    }
}

class known_hosts {
    file { 'known_hosts.sh':
        path => '/usr/local/bin/known_hosts.sh',
        mode => 500,
        owner => root,
        group => root,
        source => 'puppet:///files/known_hosts.sh'
    }

    exec { '/usr/local/bin/known_hosts.sh -y':
        subscribe => File['known_hosts.sh'],
        refreshonly => true,
    }
}

class place {
    group { 'place':
        ensure => 'present',
        gid => 10010,
    }

    file { '/place':
        ensure => directory,
        mode => '+t,ug+rwx,o-rw,o+x',
        owner => root,
        group => place,
    }
}

class hadoop_base inherits place {
    file { '/etc/hadoop/hadoop-env.sh':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/hadoop-env.sh',
    }

    file { '/etc/hadoop/masters':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/masters',
    }

    file { '/etc/hadoop/core-site.xml':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/core-site.xml',
    }

    file { '/etc/hadoop/mapred-site.xml':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/mapred-site.xml',
    }

    file { '/place/hadoop':
        ensure => directory,
        mode => 770,
        owner => hdfs,
        group => hadoop,
    }

    file { '/place/hadoop/tmp':
        ensure => directory,
        mode => 770,
        owner => hdfs,
        group => hadoop,
    }

    file { '/place/hadoop/name':
        ensure => directory,
        mode => 700,
        owner => hdfs,
        group => hadoop,
    }

    file { '/place/hadoop/data':
        ensure => directory,
        mode => 755,
        owner => hdfs,
        group => hadoop,
    }

    file { '/place/hadoop/pid':
        ensure => directory,
        mode => 775,
        owner => hdfs,
        group => hadoop,
    }

    file { '/place/hadoop/run':
        ensure => directory,
        mode => 775,
        owner => mapred,
        group => hadoop,
    }

    file { '/place/hadoop/logs':
        ensure => directory,
        mode => 775,
        owner => mapred,
        group => hadoop,
    }
}

class hadoop_slave inherits hadoop_base {
    file { '/etc/hadoop/hdfs-site.xml':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/hdfs-site-slave.xml',
    }

    service { 'hadoop-datanode':
        ensure => running,
        enable => true,
        subscribe => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ],

        require => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ]
    }

    service { 'hadoop-tasktracker':
        ensure => running,
        enable => true,
        subscribe => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ],

        require => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ]
    }
}

class hadoop_master inherits hadoop_base {
    file { '/etc/hadoop/hdfs-site.xml':
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/hadoop/hdfs-site.xml',
    }

    file { '/etc/hadoop/hosts.exclude':
        ensure => present,
        owner => root,
        group => root,
        mode => 644,
    }

    service { 'hadoop-namenode':
        ensure => running,
        enable => true,
        subscribe => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ],

        require => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ]
    }

    service { 'hadoop-jobtracker':
        ensure => running,
        enable => true,
        subscribe => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ],

        require => [
            File['/etc/hadoop/hadoop-env.sh'],
            File['/etc/hadoop/masters'],
            File['/etc/hadoop/core-site.xml'],
            File['/etc/hadoop/hdfs-site.xml'],
            File['/etc/hadoop/mapred-site.xml'],

            File['/place'],
            File['/place/hadoop'],
            File['/place/hadoop/tmp'],
            File['/place/hadoop/name'],
            File['/place/hadoop/data'],
            File['/place/hadoop/pid'],
            File['/place/hadoop/run'],
            File['/place/hadoop/logs'],
        ]
    }
}

class distcc {
    package { 'distcc':
        ensure => installed,
    }

    file { 'distcc.sh':
        path => '/usr/local/bin/distcc.sh',
        mode => 500,
        owner => root,
        group => root,
        source => 'puppet:///files/distcc.sh',

        require => Service['slurm-llnl'],
    }

    cron { 'distcc-hosts':
        command => '/usr/local/bin/distcc.sh',
        user => root,
        hour => '*',
        minute => '*',
        require => [Package['distcc'], File['distcc.sh']],
    }
}

class btsync {
    if $architecture == 'amd64' {
        file { 'btsync':
            path => '/usr/local/bin/btsync',
            ensure => present,
            mode => '755',
            owner => root,
            group => root,
            source => 'puppet:///files/btsync_amd64',
        }
    }
    elsif $architecture == 'i386' {
        file { 'btsync':
            path => '/usr/local/bin/btsync',
            ensure => present,
            mode => '755',
            owner => root,
            group => root,
            source => 'puppet:///files/btsync_i386',
        }
    }
}

class xen {
    file { '/etc/default/xendomains':
        ensure => present,
        mode => '644',
        owner => root,
        group => root,
        source => 'puppet:///files/xendomains',
    }

    file { '/usr/local/bin/xendomains.py':
        ensure => present,
        mode => '755',
        owner => root,
        group => root,
        source => 'puppet:///files/xendomains.py',
    }
}

class userqwer {
    user { 'user':
        ensure => present,
        home => '/home/user',
        shell => '/bin/bash',
        password => '$1$DWV8BrMQ$v8TnXmLTDA38CiVBEMndf.',
    }
}

class ldap {
    file { '/etc/ssl/dc3.cer':
        ensure => present,
        mode => '644',
        owner => root,
        group => root,
        source => 'puppet:///files/dc3.cer',
    }
}

class nvidia {
    file { '/etc/X11/xorg.conf':
        ensure => present,
        mode => '644',
        owner => root,
        group => root,
        source => 'puppet:///files/nvidia.xorg.conf',
        notify => Service['gdm'],
    }
}

class pam_mount {
    package { 'libpam-mount':
        ensure => installed,
        notify => Service['gdm'],
    }

    file { '/etc/security/pam_mount.conf.xml':
        require => Package['libpam-mount'],
        ensure => present,
        mode => '644',
        owner => root,
        group => root,
        source => 'puppet:///files/pam_mount.conf.xml',
    }
}

class sp {
    file { '/usr/local/bin/sp':
        ensure => present,
        mode => '755',
        owner => root,
        group => root,
        source => 'puppet:///files/sp',
    }

    file { '/etc/sudoers.d':
        ensure => directory,
        mode => '644',
        owner => root,
        group => root,
    }

    file { '/etc/sudoers.d/sp':
        ensure => present,
        mode => '440',
        owner => root,
        group => root,
        source => 'puppet:///files/sp.sudoers',
        require => File['/etc/sudoers.d'],
    }

    exec { '/usr/local/bin/sp':
        subscribe => File['/usr/local/bin/sp'],
        refreshonly => true,
    }
}

class teachers {
    file { '/etc/sudoers.d/teachers':
        ensure => present,
        mode => '440',
        owner => root,
        group => root,
        source => 'puppet:///files/teachers.sudoers',
        require => File['/etc/sudoers.d'],
    }
}

class exim_default {
    package { 'exim4-daemon-light':
        ensure => installed,
    }

    service { 'exim4': }

    file { '/etc/exim4/update-exim4.conf.conf':
        require => Package['exim4-daemon-light'],
        ensure => present,
        mode => '644',
        owner => root,
        group => root,
        source => 'puppet:///files/update-exim4.conf.conf',
    }

    exec { '/usr/sbin/update-exim4.conf':
        subscribe => File['/etc/exim4/update-exim4.conf.conf'],
        refreshonly => true,
        notify => Service['exim4'],
    }
}

class exchange {
    file { '/exchange':
        owner => root,
        group => root,
        mode => '777',
        ensure => directory,
    }

    package { 'cifs-utils':
        ensure => installed,
    }

    package { 'winbind':
        ensure => absent,
    }

    mount { '/exchange':
        require => [File['/exchange'], Package['cifs-utils']],
        ensure => mounted,
        atboot => true,
        device => '//exchange/exchange',
        fstype => 'cifs',
        options => 'guest,file_mode=0666,dir_mode=0777',
    }
}

class hwraid {
    package { 'sudo':
        ensure => installed,
    }

    apt::key { '23B3D3B4':
        key_source => 'http://hwraid.le-vert.net/debian/hwraid.le-vert.net.gpg.key',
    }

    apt::source { 'hwraid':
        location => 'http://hwraid.le-vert.net/debian',
        repos => 'main',
        require => Apt::Key['23B3D3B4'],
    }
}

class megaraid inherits hwraid {
    package { 'megacli':
        ensure => installed,
        require => Apt::Source['hwraid'],
    }

    file { '/usr/lib/nagios/plugins/check_megaraid_sas':
        ensure => present,
        mode => 755,
        owner => root,
        group => root,
        source => "puppet:///files/check_megaraid_sas",
    }
}

class adaptecraid inherits hwraid {
    package { 'arcconf':
        ensure => installed,
        require => Apt::Source['hwraid'],
    }
}

class db {
    package { 'nagios-plugins-standard':
        ensure => installed,
        install_options => ['--no-install-recommends'],
    }
}

class mysql inherits db {
}

class postgresql inherits db {
    package { 'sudo':
        ensure => installed,
    }

    file { '/etc/sudoers.d/nagios-postgresql':
        require => Package['sudo'],
        ensure => present,
        mode => '440',
        owner => root,
        group => root,
        content =>
            'nagios ALL=(postgres) NOPASSWD: /usr/lib/nagios/plugins/check_pgsql
            ',
    }
}

class smartmon {
    package { 'smartmontools':
        ensure => installed,
    }

    file { '/usr/lib/nagios/plugins/check_smartmon':
        mode => 755,
        owner => root,
        group => root,
        source => "puppet:///files/check_smartmon",
    }
}

class localtime {
    exec { 'set-localtime':
        command => '/bin/sed -i "s/UTC/LOCAL/" /etc/adjtime && \
            /etc/init.d/hwclock.sh reload && \
            sntp -s urgu.org',
        onlyif => '/bin/grep UTC /etc/adjtime',
    }
}

class ad inherits ldap {
    package { 'nslcd': ensure => installed }

    service { 'nslcd':
        ensure => 'running',
        enable => 'true',
        require => Package['nslcd'],
    }

    file { '/etc/nslcd.conf':
        require => Package['nslcd'],
        mode => 640,
        owner => root,
        group => nslcd,
        source => 'puppet:///files/nslcd.conf',
        notify => Service['nslcd'],
    }

    package { 'nscd': ensure => installed }

    service { 'nscd':
        ensure => 'running',
        enable => 'true',
        require => Package['nscd'],
    }

    file { '/etc/nscd.conf':
        require => Package['nscd'],
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/nscd.conf',
        notify => Service['nscd'],
    }

    augeas { 'ldap':
        require => [Package['nslcd']],
        context => '/files/etc/nsswitch.conf',
        changes => ["set database[. = 'passwd']/service[1] files",
                    "set database[. = 'passwd']/service[2] ldap",
                    "set database[. = 'group']/service[1] files",
                    "set database[. = 'group']/service[2] ldap",
                    "set database[. = 'shadow']/service[1] files",
                    "set database[. = 'shadow']/service[2] ldap"],
        notify => Service['nscd'],
    }

    package { 'krb5-user':
        ensure => installed,
    }

    file { '/etc/krb5.conf':
        require => Package['krb5-user'],
        mode => 644,
        owner => root,
        group => root,
        source => 'puppet:///files/krb5.conf',
    }
}

class sensors {
    package { 'lm-sensors':
        ensure => installed,
    }
}

class essential_soft {
    {% for pkg in ['ntp', 'arping', 'telnet', 'tcpdump', 'mtr-tiny',
                   'vim', 'git', 'screen', 'augeas-tools',
                   'ca-certificates', 'atop', 'htop', 'iotop', 'lsof', 'jnettop',
                   'parted', 'gnu-fdisk', 'gdisk', 'lvm2', 'kpartx',
                   'iperf', 'bridge-utils', 'vlan', 'ifenslave-2.6',
                   'stress', 'hdparm', 'sysbench',
                   'p7zip-full', 'gzip', 'lzma', 'zip', 'unzip'] %}
    package { '{{ pkg }}': ensure => installed }
    {% endfor %}
}

class backup {
    file {'/usr/local/bin/backup.sh':
        mode => 555,
        owner => root,
        group => root,
        source => "puppet:///files/backup.sh",
    }
}

{% for host in state.hosts %}
node '{{ host.name }}' {
    {% if 'hadoop-master' in host.props['services'] %}
        include hadoop_master
        $check_hadoop_master = 'true'

        file { '/etc/hadoop/rack.sh':
          mode => 755,
          owner => root,
          group => root,
          content => '#!/usr/bin/env bash
            declare -A rack
            {% for host in state.hosts %}
              {% if 'hadoop_rack' in host.props %}
                rack[{{ host.addr }}]="{{ host.props['hadoop_rack']}}"
              {% endif %}
            {% endfor %}
            echo $@ >> /tmp/rack.log
            for ip in "$@"; do
              echo -n "/${rack[$ip]:-default-rack} "
            done
          ',
        }
    {% endif %}

    {% if 'slurm-master' in host.props['services'] %}
        $check_slurm_master = 'true'
    {% endif %}

    {% if 'unix' in host.services %}
        include essential_soft

        {% if 'backups' in host.props %}
            group { 'backupers': ensure => present, }

            {% set backup_root = host.props['backups'][0] %}

            file { '{{ backup_root }}':
                require => Group['backupers'],
                ensure => directory,
                owner => root,
                group => backupers,
                mode => 770,
            }

            file { '/usr/local/bin/backuper.sh':
                require => Group['backupers'],
                ensure => present,
                owner => root,
                group => backupers,
                mode => 755,
                source => 'puppet:///files/backuper.sh',
            }

            {% for bhost, bprops in host.props['backups'][1].iteritems() %}
                {% set backed_host = state.find(bhost) %}
                {% set backup_user = 'backup-{}'.format(backed_host.name) %}
                {% set backup_dir = '{}/{}'.format(backup_root,
                                                   backed_host.name) %}
                user { '{{ backup_user }}':
                    require => Group['backupers'],
                    home => '{{ backup_dir }}',
                    ensure => present,
                    gid => backupers,
                }

                mailalias { '{{ backup_user }}':
                    name => '{{ backup_user }}',
                    ensure => present,
                    recipient => '{{ state.users[host.props['admin']].email }}'
                }

                file { '{{ backup_dir }}':
                    require => User['{{ backup_user }}'],
                    ensure => directory,
                    owner => '{{ backup_user }}',
                    group => backupers,
                    mode => 700,
                }

                file { '{{ backup_dir }}/.ssh':
                    require => User['{{ backup_user }}'],
                    ensure => directory,
                    owner => '{{ backup_user }}',
                    group => backupers,
                    mode => 700,
                }
            {% endfor %}

            {% for item in state.get_backup_schedule(host.name) %}
                {% set backed_host = item.host %}
                {% set backup_user = 'backup-{}'.format(backed_host.name) %}
                cron { 'backup-{{ backed_host }}-{{ item.day }}':
                    user => '{{ backup_user }}',
                    command => '/usr/local/bin/backuper.sh "{{ backed_host }}" {{ item.btype }}{%
                        for bprop in item.bprops %} {{ bprop }}{% endfor %}',

                    weekday => '{{ item.day }}',
                    hour => '{{ item.hour }}',
                    minute => '{{ item.minute }}',
                }
            {% endfor %}

            file { '/usr/local/bin/backup-auth.sh':
                ensure => present,
                owner => root,
                group => backupers,
                mode => 750,
                content => '#!/usr/bin/env bash
set -e
case "$1" in
    generate_id)
        keyfile="$HOME/.ssh/id_rsa"
        if ! [[ -f "$keyfile" ]]; then
            ssh-keygen -b 4096 -f "$keyfile" -N ""
        fi
    ;;
    *)
        for host in {%- for bhost in
                        host.props['backups'][1].iterkeys() -%}
                    {{ ' {}'.format(state.find(bhost).name) }}
                    {%- endfor -%}; do
            su "backup-$host" /usr/bin/env bash -c "$0 generate_id"
            pubkey=$(su "backup-$host" -c \'cat "$HOME/.ssh/id_rsa.pub"\')
            if ssh "$host" "[[ -f .ssh/authorized_keys ]]"; then
                ssh "$host" sed -i "/${pubkey##* }/d" .ssh/authorized_keys
                echo "$pubkey" | ssh "$host" "cat >> .ssh/authorized_keys"
            else
                echo "no .ssh/authorized_keys on $host" 1>&2
            fi
        done
    ;;
esac
                ',
            }
        {% endif %}

        {% if 'modules' in host.props %}
            exec { '/etc/init.d/kmod start':
                refreshonly => true,
            }

            {% for module in host.props['modules'] %}
                augeas { "module-{{ module }}" :
                    context => "/files/etc/modules",
                    changes => ["clear {{ module }}"],
                    notify => Exec['/etc/init.d/kmod start'],
                }
            {% endfor %}
        {% endif %}

        {% if 'sensors' in host.props %}
            $check_sensors = '{{ '+'.join(host.props['sensors']) }}'
            file { '/usr/lib/nagios/plugins/check_sensors.py':
                mode => 755,
                owner => root,
                group => root,
                source => "puppet:///files/check_sensors.py",
            }
            include sensors
        {% endif %}

        {% if 'sudoers' in host.props %}
            augeas { 'sudo':
                context => '/files/etc/group/sudo',
                changes => ['rm user',
                {% for user in host.props['sudoers'] %}
                            'set user[last()] {{ user }}',
                {% endfor %}
                           ],
            }
        {% endif %}

        $nrpe_allowed = '{{ ([state.get_nagios(host.addr)]
            + host.props.get('nrpe_allowed', []))|join(',') }}'
        {% if host.addr %}
            $nrpe_listen = '{{ host.addr }}'
        {% else %}
            $nrpe_listen = '0.0.0.0'
        {% endif %}
        {% if 'openvz' in host.services %}
            $nrpe_check_disk_root = 'true'
            $nrpe_procs_warn = '700'
            $nrpe_procs_crit = '1000'
        {% elif 'xen' in host.services %}
            $nrpe_procs_warn = '300'
            $nrpe_procs_crit = '500'
        {% else %}
            $nrpe_procs_warn = '250'
            $nrpe_procs_crit = '400'
        {% endif %}
        {% if 'megaraid' in host.props['services'] %}
            $check_megaraid = 'true'
        {% endif %}

        {% if 'docker' in host.services %}
        $check_docker = 'true'
        {% endif %}

        {% if 'managed' in host.props %}
            {% if 'userqwer' in host.services %}
                include userqwer
                $userqwer = 'true'
            {% endif %}
            $check_apt = 'false'

            {% if 'localtime' in host.props %}
            include localtime
            {% endif %}

            {% if 'hadoop_rack' in host.props %}
            $hadoop_rack = '{{ host.props['hadoop_rack']}}'
            {% else %}
            $hadoop_rack = 'default-rack'
            {% endif %}
            include hadoop_slave
            $check_hadoop_slave = 'true'

            include smartmon
            $check_smartmon = 'true'

            $check_slurm_node = 'true'

            include exchange
            include managed-apt
            include managed-ssh
            include managed-update
            include pam_mount
            include smb
            include teachers

            package { 'cron-apt': ensure => absent }
            file { '/etc/hostname': content => '{{ host.name }}' }
            file { '/etc/mailname':
                content => '{{ host.name }}',
                notify => Service['exim4'],
            }
        {% else %}
            $cron_apt_admin = '{%- if 'admin' in host.props -%}
                                   {{ state.users[host.props['admin']].email }}
                               {%- else -%}
                                   root
                               {%- endif -%}'
            $check_apt = 'true'
            {% if 'autoupdate' in host.services %}
                $autoupdate = 'true'
            {% endif %}
            include cron-apt
            include ssh
        {% endif %}

        {% if 'exim_default' in host.services or 'managed' in host.props %}
            include exim_default
        {% endif %}

        {% if 'managed-apt' in host.services %}
            include managed-apt
        {% endif %}

        {% if 'nagios' in host.services %}
            $check_nagios = 'true'
        {% else %}
            $check_nagios = 'false'
        {% endif %}

        {% if 'mysql' in host.services %}
            include mysql
        {% endif %}

        {% if 'postgresql' in host.services %}
            $check_postgresql = 'true'
            include postgresql
        {% endif %}

        {% if 'xen' in host.services %}
            include xen
        {% endif %}

        {% if 'nvidia' in host.services %}
            include nvidia
        {% endif %}

        {% if 'admin' in host.props %}
            mailalias { 'root':
                name => root,
                ensure => present,
                recipient => '{{ state.users[host.props['admin']].email }}'
            }

            $mdadm_admin = '{{ state.users[host.props['admin']].email }}'
            include mdadm
        {% endif %}

        {% if 'megaraid' in host.services %}
            include megaraid
        {% elif 'adaptecraid' in host.services %}
            include adaptecraid
        {% endif %}

        {% if 'ups' in host.props %}
            {% if host.props['ups'] is string %}
                $upsname = '{{ state.find(host.props['ups']).name }}'
                $upsip = '{{ state.find(host.props['ups']).addr }}'
            {% else %}
                $upsname = '{{ state.find(host.props['ups'][0]).name }}'
                $upsip = '{{ state.find(host.props['ups'][0]).addr }}'
            {% endif %}

            include ups
        {% endif %}

        {% if 'smb' in host.services %}
            include smb
        {% endif %}

        {% if 'ad' in host.services %}
            include ad
        {% endif %}

        $host = '{{ state.get_canonical_hostname(host) }}'
        $nsca_host = '{{ state.defaults.nagios }}'
        $puppet_random_host = 'urgu.org'
        include random

        include backup
        include btsync
        include distcc
        include gdm
        include known_hosts
        include ldap
        include nrpe
        include runc_ca
        include slurm
        include sp
    {% endif %}
}
{% endfor %}
